{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":9334901,"sourceType":"datasetVersion","datasetId":5656537},{"sourceId":9350496,"sourceType":"datasetVersion","datasetId":5667927},{"sourceId":9358802,"sourceType":"datasetVersion","datasetId":5674059},{"sourceId":9457466,"sourceType":"datasetVersion","datasetId":5749314}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport time\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import OneHotEncoder\nimport polars as pl\n\nimport cv2\nfrom PIL import Image\nimport io\nimport h5py\nimport math \n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nimport timm\n\nfrom sklearn.metrics import hamming_loss, f1_score, roc_curve, auc, classification_report\nfrom sklearn.preprocessing import binarize\nfrom sklearn.model_selection import GroupShuffleSplit\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-22T19:23:35.808318Z","iopub.execute_input":"2024-09-22T19:23:35.808765Z","iopub.status.idle":"2024-09-22T19:24:01.348142Z","shell.execute_reply.started":"2024-09-22T19:23:35.808719Z","shell.execute_reply":"2024-09-22T19:24:01.347131Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/check_version.py:49: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set up device and random seed\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\nrandom_seed = 42\nrandom.seed(random_seed)\ntorch.manual_seed(random_seed)\nnp.random.seed(random_seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n#number of epochs to train for\nnum_epochs = 50\n\n#train entire model vs. just the classifier\nfreeze_base_model = False  #didn't get good results\n\n# if this is set to true - full model is only generated as part of scoring (quick_train_record_count used)\n# this saves GPU quota - but saved model won't reflect what was scored...\nfull_train_only_when_scoring = False  #must be False to save full model!\nquick_train_record_count = 50000   #need to get at least some positive cases even for test run","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:01.353282Z","iopub.execute_input":"2024-09-22T19:24:01.353575Z","iopub.status.idle":"2024-09-22T19:24:01.416890Z","shell.execute_reply.started":"2024-09-22T19:24:01.353542Z","shell.execute_reply":"2024-09-22T19:24:01.415862Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla P100-PCIE-16GB\nNumber of GPUs: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"bad_ids = ['ISIC_0091661', 'ISIC_0157300', 'ISIC_0164004', 'ISIC_0225902',\n       'ISIC_0247991', 'ISIC_0253221', 'ISIC_0275821', 'ISIC_0276162',\n       'ISIC_0321282', 'ISIC_0512487', 'ISIC_0573025', 'ISIC_0749379',\n       'ISIC_0887823', 'ISIC_1142893', 'ISIC_1180656', 'ISIC_1194950',\n       'ISIC_1280179', 'ISIC_1334224', 'ISIC_1338006', 'ISIC_1443812',\n       'ISIC_1488609', 'ISIC_1618438', 'ISIC_1716141', 'ISIC_1755348',\n       'ISIC_1882290', 'ISIC_2066646', 'ISIC_2082383', 'ISIC_2153489',\n       'ISIC_2185868', 'ISIC_2301755', 'ISIC_2325643', 'ISIC_2326801',\n       'ISIC_2501464', 'ISIC_2563846', 'ISIC_2592061', 'ISIC_2649560',\n       'ISIC_2842612', 'ISIC_3019770', 'ISIC_3245832', 'ISIC_3355799',\n       'ISIC_3606755', 'ISIC_3673016', 'ISIC_3856578', 'ISIC_3902330',\n       'ISIC_3905526', 'ISIC_3915836', 'ISIC_3970343', 'ISIC_4029206',\n       'ISIC_4172573', 'ISIC_4244859', 'ISIC_4435163', 'ISIC_4590578',\n       'ISIC_4628194', 'ISIC_4723477', 'ISIC_4794318', 'ISIC_4837618',\n       'ISIC_4859161', 'ISIC_4884516', 'ISIC_4992507', 'ISIC_5129222',\n       'ISIC_5374420', 'ISIC_5446672', 'ISIC_5644802', 'ISIC_5678455',\n       'ISIC_5687461', 'ISIC_5873888', 'ISIC_5874842', 'ISIC_5938732',\n       'ISIC_5990814', 'ISIC_6021059', 'ISIC_6145237', 'ISIC_6233830',\n       'ISIC_6265900', 'ISIC_6290217', 'ISIC_6303557', 'ISIC_6347423',\n       'ISIC_6415548', 'ISIC_6443962', 'ISIC_6505439', 'ISIC_6731439',\n       'ISIC_6757661', 'ISIC_6794549', 'ISIC_6796625', 'ISIC_6838918',\n       'ISIC_6931102', 'ISIC_7028157', 'ISIC_7348810', 'ISIC_7358578',\n       'ISIC_7386083', 'ISIC_7445245', 'ISIC_7478620', 'ISIC_7546705',\n       'ISIC_7805616', 'ISIC_8091604', 'ISIC_8182531', 'ISIC_8278020',\n       'ISIC_8379868', 'ISIC_8537711', 'ISIC_8644028', 'ISIC_8653720',\n       'ISIC_8755758', 'ISIC_9118564', 'ISIC_9156598', 'ISIC_9342841',\n       'ISIC_9458222', 'ISIC_9476302', 'ISIC_9546926', 'ISIC_9611761',\n       'ISIC_9645582', 'ISIC_9675639', 'ISIC_9713969', 'ISIC_9758190',\n        'ISIC_0235999', 'ISIC_0633292', 'ISIC_1137305', 'ISIC_1203239',\n       'ISIC_1243149', 'ISIC_1449578', 'ISIC_1642217', 'ISIC_1939971',\n       'ISIC_1992066', 'ISIC_1997122', 'ISIC_3113900', 'ISIC_3809050',\n       'ISIC_3939743', 'ISIC_4368177', 'ISIC_4704252', 'ISIC_4711591',\n       'ISIC_5063101', 'ISIC_5105473', 'ISIC_5290013', 'ISIC_5859200',\n       'ISIC_6050068', 'ISIC_7083114', 'ISIC_7134832', 'ISIC_7210490',\n       'ISIC_8397035', 'ISIC_8644948', 'ISIC_8829281', 'ISIC_8897434',\n       'ISIC_9183347', 'ISIC_9200535', 'ISIC_9441116', 'ISIC_9500444']","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:01.418319Z","iopub.execute_input":"2024-09-22T19:24:01.419124Z","iopub.status.idle":"2024-09-22T19:24:01.429489Z","shell.execute_reply.started":"2024-09-22T19:24:01.419086Z","shell.execute_reply":"2024-09-22T19:24:01.428405Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"num_cols = [\n    'age_approx',                        # Approximate age of patient at time of imaging.\n    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n    'tbp_lv_A',                          # A inside  lesion.+\n    'tbp_lv_Aext',                       # A outside lesion.+\n    'tbp_lv_B',                          # B inside  lesion.+\n    'tbp_lv_Bext',                       # B outside lesion.+\n    'tbp_lv_C',                          # Chroma inside  lesion.+\n    'tbp_lv_Cext',                       # Chroma outside lesion.+\n    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n    'tbp_lv_Hext',                       # Hue outside lesion.+\n    'tbp_lv_L',                          # L inside lesion.+\n    'tbp_lv_Lext',                       # L outside lesion.+\n    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaLB',                    #\n    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n    'tbp_lv_eccentricity',               # Eccentricity.+\n    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n]\n\nnew_num_cols = [\n    'lesion_size_ratio',                 # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n    'lesion_shape_index',                # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n    'hue_contrast',                      # tbp_lv_H                - tbp_lv_Hext              abs\n    'luminance_contrast',                # tbp_lv_L                - tbp_lv_Lext              abs\n    'lesion_color_difference',           # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt\n    'border_complexity',                 # tbp_lv_norm_border      + tbp_lv_symm_2axis\n    'color_uniformity',                  # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n\n    'position_distance_3d',              # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n    'perimeter_to_area_ratio',           # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n    'area_to_perimeter_ratio',           # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n    'lesion_visibility_score',           # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n    'symmetry_border_consistency',       # tbp_lv_symm_2axis       * tbp_lv_norm_border\n    'consistency_symmetry_border',       # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n\n    'color_consistency',                 # tbp_lv_stdL             / tbp_lv_Lext\n    'consistency_color',                 # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n    'size_age_interaction',              # clin_size_long_diam_mm  * age_approx\n    'hue_color_std_interaction',         # tbp_lv_H                * tbp_lv_color_std_mean\n    'lesion_severity_index',             # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n    'shape_complexity_index',            # border_complexity       + lesion_shape_index\n    'color_contrast_index',              # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n\n    'log_lesion_area',                   # tbp_lv_areaMM2          + 1  np.log\n    'normalized_lesion_size',            # clin_size_long_diam_mm  / age_approx\n    'mean_hue_difference',               # tbp_lv_H                + tbp_lv_Hext    / 2\n    'std_dev_contrast',                  # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n    'color_shape_composite_index',       # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n    'lesion_orientation_3d',             # tbp_lv_y                , tbp_lv_x  np.arctan2\n    'overall_color_difference',          # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n\n    'symmetry_perimeter_interaction',    # tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n    'comprehensive_lesion_index',        # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n    'color_variance_ratio',              # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n    'border_color_interaction',          # tbp_lv_norm_border      * tbp_lv_norm_color\n    'border_color_interaction_2',\n    'size_color_contrast_ratio',         # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n    'age_normalized_nevi_confidence',    # tbp_lv_nevi_confidence  / age_approx\n    'age_normalized_nevi_confidence_2',\n    'color_asymmetry_index',             # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n\n    'volume_approximation_3d',           # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n    'color_range',                       # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n    'shape_color_consistency',           # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n    'border_length_ratio',               # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n    'age_size_symmetry_index',           # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n    'index_age_size_symmetry',           # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n]\n\ncat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution']\nnorm_cols = [f'{col}_patient_norm' for col in num_cols + new_num_cols]\nspecial_cols = ['count_per_patient']\n\n#norm_cols += image_cols\nfeature_cols = num_cols + cat_cols + special_cols + new_num_cols # norm_cols\n\nprint(f\"...{len(feature_cols)} FEATURES IN TOTAL...\")\nprint(f\"\\t– num_cols: {len(num_cols)}\");\nprint(f\"\\t– new_num_cols: {len(new_num_cols)}\");\nprint(f\"\\t– cat_cols: {len(cat_cols)}\");\nprint(f\"\\t– norm_cols: {len(norm_cols)}\");\nprint(f\"\\t– special_cols: {len(special_cols)}\");","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:01.432252Z","iopub.execute_input":"2024-09-22T19:24:01.432619Z","iopub.status.idle":"2024-09-22T19:24:01.449344Z","shell.execute_reply.started":"2024-09-22T19:24:01.432585Z","shell.execute_reply":"2024-09-22T19:24:01.448442Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"...83 FEATURES IN TOTAL...\n\t– num_cols: 34\n\t– new_num_cols: 42\n\t– cat_cols: 6\n\t– norm_cols: 76\n\t– special_cols: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"def read_data(path):\n    small_value = 1e-5  # A small constant to avoid division by zero\n    return (\n        pl.read_csv(path)\n        .with_columns(\n            pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n        )\n        .with_columns(\n            pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()),  # Impute NaN values with median\n        )\n        .with_columns(\n            lesion_size_ratio              = pl.when(pl.col('clin_size_long_diam_mm') != 0).then(pl.col('tbp_lv_minorAxisMM') / (pl.col('clin_size_long_diam_mm') + small_value)),\n            lesion_shape_index             = pl.when(pl.col('tbp_lv_perimeterMM') != 0).then(pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2 + small_value)),\n            hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n            luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n            lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n            border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n            color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + small_value),\n        )\n        .with_columns(\n            position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n            perimeter_to_area_ratio        = pl.when(pl.col('tbp_lv_areaMM2') != 0).then(pl.col('tbp_lv_perimeterMM') / (pl.col('tbp_lv_areaMM2') + small_value)),\n            area_to_perimeter_ratio        = pl.when(pl.col('tbp_lv_perimeterMM') != 0).then(pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') + small_value)),\n            lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n            combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n            symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n            consistency_symmetry_border    = pl.when((pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')) != 0).then(pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border') + small_value)),\n        )\n        .with_columns(\n            color_consistency              = pl.when(pl.col('tbp_lv_Lext') != 0).then(pl.col('tbp_lv_stdL') / (pl.col('tbp_lv_Lext') + small_value)),\n            consistency_color              = pl.when((pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')) != 0).then(pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext') + small_value)),\n            size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n            hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n            lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n            shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n            color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n        )\n        .with_columns(\n            log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n            normalized_lesion_size         = pl.when(pl.col('age_approx') != 0).then(pl.col('clin_size_long_diam_mm') / (pl.col('age_approx') + small_value)),\n            mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n            std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n            color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n            lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n            overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n        )\n        .with_columns(\n            symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n            comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n            color_variance_ratio           = pl.when(pl.col('tbp_lv_stdLExt') != 0).then(pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_stdLExt') + small_value)),\n            border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n            border_color_interaction_2     = pl.when((pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')) != 0).then(pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + small_value)),\n            size_color_contrast_ratio      = pl.when(pl.col('tbp_lv_deltaLBnorm') != 0).then(pl.col('clin_size_long_diam_mm') / (pl.col('tbp_lv_deltaLBnorm') + small_value)),\n            age_normalized_nevi_confidence = pl.when(pl.col('age_approx') != 0).then(pl.col('tbp_lv_nevi_confidence') / (pl.col('age_approx') + small_value)),\n            age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n            color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n        )\n        .with_columns(\n            volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n            color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n            shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n            border_length_ratio            = pl.when(pl.col('tbp_lv_areaMM2') != 0).then(pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt() + small_value)),\n            age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n            index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n        )\n        .with_columns(\n            ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + small_value)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n        )\n        .with_columns(\n            count_per_patient = pl.col('isic_id').count().over('patient_id'),\n        )\n        .with_columns(\n            pl.col(cat_cols).cast(pl.Categorical)\n        )\n        .to_pandas()\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:01.452833Z","iopub.execute_input":"2024-09-22T19:24:01.453202Z","iopub.status.idle":"2024-09-22T19:24:01.488344Z","shell.execute_reply.started":"2024-09-22T19:24:01.453167Z","shell.execute_reply":"2024-09-22T19:24:01.487390Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n# def read_data(path):\n#     return (\n#         pl.read_csv(path)\n#         .with_columns(\n#             pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n#         )\n#         .with_columns(\n#             pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()), # You may want to impute test data with train\n#         )\n#         .with_columns(\n#             lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n#             lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n#             hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n#             luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n#             lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n#             border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n#             color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + err),\n#         )\n#         .with_columns(\n#             position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n#             perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n#             area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n#             lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n#             combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n#             symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n#             consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n#         )\n#         .with_columns(\n#             color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n#             consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n#             size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n#             hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n#             lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n#             shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n#             color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n#         )\n#         .with_columns(\n#             log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n#             normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n#             mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n#             std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n#             color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n#             lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n#             overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n#         )\n#         .with_columns(\n#             symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n#             comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n#             color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n#             border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n#             border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n#             size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n#             age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n#             age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n#             color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n#         )\n#         .with_columns(\n#             volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n#             color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n#             shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n#             border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n#             age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n#             index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n#         )\n#         .with_columns(\n#             ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + err)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n#         )\n#         .with_columns(\n#             count_per_patient = pl.col('isic_id').count().over('patient_id'),\n#         )\n#         .with_columns(\n#             pl.col(cat_cols).cast(pl.Categorical)\n#         )\n#         .to_pandas()\n#     )","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:01.489918Z","iopub.execute_input":"2024-09-22T19:24:01.490273Z","iopub.status.idle":"2024-09-22T19:24:01.503201Z","shell.execute_reply.started":"2024-09-22T19:24:01.490239Z","shell.execute_reply":"2024-09-22T19:24:01.502312Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess(df_train, df_test):\n    global cat_cols\n\n    encoder = OneHotEncoder(sparse_output=False, dtype=np.int32, handle_unknown='ignore')\n    encoder.fit(df_train[cat_cols])\n\n    new_cat_cols = [f'onehot_{i}' for i in range(len(encoder.get_feature_names_out()))]\n\n    df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n    df_train[new_cat_cols] = df_train[new_cat_cols].astype('category')\n\n    df_test[new_cat_cols] = encoder.transform(df_test[cat_cols])\n    df_test[new_cat_cols] = df_test[new_cat_cols].astype('category')\n\n\n    for col in cat_cols:\n        feature_cols.remove(col)\n\n    feature_cols.extend(new_cat_cols)\n    cat_cols = new_cat_cols\n\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:01.504724Z","iopub.execute_input":"2024-09-22T19:24:01.505330Z","iopub.status.idle":"2024-09-22T19:24:01.517179Z","shell.execute_reply.started":"2024-09-22T19:24:01.505288Z","shell.execute_reply":"2024-09-22T19:24:01.516203Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# df_train = pd.read_csv(\"/kaggle/input/isic-2024-challenge/train-metadata.csv\")\n# df_test = pd.read_csv(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\")\n\nTEST_HDF5_FILE_PATH = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\nerr = 1e-5\nid_col = 'isic_id'\n\ntrain_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\ntest_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\ndf_train = read_data(train_path)\ndf_train = df_train[~df_train['isic_id'].isin(bad_ids)].reset_index(drop=True)\ndf_test = read_data(test_path)\n\n# df_train = df_train.merge(df_selfclean, on=[\"isic_id\", \"patient_id\"])\n# df_train = df_train[(df_train['target'] == 1) | (df_train['irrelevant_score'] <= 0.99834)].reset_index(drop=True)\n\n\n\n# df_train = pd.read_csv(\"/content/train-metadata.csv\")\n# df_train = df_train[~df_train['isic_id'].isin(bad_ids)].reset_index(drop=True)\n\n\nnum_folds = 5\n\ngkf = GroupKFold(n_splits=num_folds)\n\ndf_train[\"fold\"] = -1\nfor idx, (train_idx, val_idx) in enumerate(gkf.split(df_train, df_train[\"target\"], groups=df_train[\"patient_id\"])):\n    df_train.loc[val_idx, \"fold\"] = idx\n\n# Add summary\nfold_summary = df_train.groupby(\"fold\")[\"patient_id\"].nunique().to_dict()\ntotal_patients = df_train[\"patient_id\"].nunique()\n\nprint(f\"Fold Summary (patients per fold):\")\nfor fold, count in fold_summary.items():\n    if fold != -1:  # Exclude the initialization value\n        print(f\"Fold {fold}: {count} patients\")\nprint(f\"Total patients: {total_patients}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:01.518481Z","iopub.execute_input":"2024-09-22T19:24:01.519210Z","iopub.status.idle":"2024-09-22T19:24:06.901945Z","shell.execute_reply.started":"2024-09-22T19:24:01.519175Z","shell.execute_reply":"2024-09-22T19:24:06.900645Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fold Summary (patients per fold):\nFold 0: 206 patients\nFold 1: 209 patients\nFold 2: 209 patients\nFold 3: 209 patients\nFold 4: 209 patients\nTotal patients: 1042\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train, df_test = preprocess(df_train, df_test)\nprint(f\"\\t– TRAIN DATAFRAME SHAPE: {df_train.shape}\");\nprint(f\"\\t– TEST DATAFRAME SHAPE: {df_test.shape}\");\n\nprint(\"\\n... READS DATA COMPLETE ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:06.903534Z","iopub.execute_input":"2024-09-22T19:24:06.904170Z","iopub.status.idle":"2024-09-22T19:24:08.292147Z","shell.execute_reply.started":"2024-09-22T19:24:06.904112Z","shell.execute_reply":"2024-09-22T19:24:08.291007Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\t– TRAIN DATAFRAME SHAPE: (400915, 223)\n\t– TEST DATAFRAME SHAPE: (3, 211)\n\n... READS DATA COMPLETE ...\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df_test[num_cols] = df_test[num_cols].replace(\"NA\", np.nan).astype(np.float64)\ndf_test[new_num_cols] = df_test[new_num_cols].replace(\"NA\", np.nan).astype(np.float64)\ndf_test[cat_cols] = df_test[cat_cols].replace(\"NA\", np.nan).astype(str)\ndf_test[special_cols] = df_test[special_cols].replace(\"NA\", np.nan).astype(np.int32)\n\ndf_test[num_cols] = df_test[num_cols].fillna(df_test[num_cols].median())\ndf_test[new_num_cols] = df_test[new_num_cols].fillna(df_test[new_num_cols].median())\ndf_test[special_cols] = df_test[special_cols].fillna(df_test[special_cols].median())\ndf_test[cat_cols] = df_test[cat_cols].fillna(df_test[cat_cols].mode().iloc[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:08.293373Z","iopub.execute_input":"2024-09-22T19:24:08.293712Z","iopub.status.idle":"2024-09-22T19:24:08.445124Z","shell.execute_reply.started":"2024-09-22T19:24:08.293679Z","shell.execute_reply":"2024-09-22T19:24:08.444026Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# df_train[num_cols] = df_train[num_cols].replace(\"NA\", np.nan).astype(np.float64)\n# df_train[cat_cols] = df_train[cat_cols].replace(\"NA\", np.nan).astype(str)\n\n# df_train[num_cols] = df_train[num_cols].fillna(df_train[num_cols].median())\n# df_train[cat_cols] = df_train[cat_cols].fillna(df_train[cat_cols].mode().iloc[0])\n\n# df_train = df_train.replace(np.inf, np.nan)\n# df_train = df_train.replace(-np.inf, np.nan)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:08.446570Z","iopub.execute_input":"2024-09-22T19:24:08.446967Z","iopub.status.idle":"2024-09-22T19:24:08.451368Z","shell.execute_reply.started":"2024-09-22T19:24:08.446903Z","shell.execute_reply":"2024-09-22T19:24:08.450331Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Set the HDF5 file path\nTRAIN_HDF5_FILE_PATH = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\n\n# are we scoring?\nscoring = False\n#check length of test data to see if we are scoring....\ntest_length = len(pd.read_csv(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\"))\nif test_length > 3:\n    scoring = True\n\nif not scoring:\n    if full_train_only_when_scoring:\n        df_train = df_train.head(quick_train_record_count)\n\nprint(\"\\nOriginal Dataset Summary:\")\nprint(f\"Total number of samples: {len(df_train)}\")\nprint(f\"Number of unique patients: {df_train['patient_id'].nunique()}\")\n\noriginal_positive_cases = df_train['target'].sum()\noriginal_total_cases = len(df_train)\noriginal_positive_ratio = original_positive_cases / original_total_cases\n\nprint(f\"Number of positive cases: {original_positive_cases}\")\nprint(f\"Number of negative cases: {original_total_cases - original_positive_cases}\")\nprint(f\"Ratio of negative to positive cases: {(original_total_cases - original_positive_cases) / original_positive_cases:.2f}:1\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:08.452919Z","iopub.execute_input":"2024-09-22T19:24:08.453292Z","iopub.status.idle":"2024-09-22T19:24:08.506874Z","shell.execute_reply.started":"2024-09-22T19:24:08.453250Z","shell.execute_reply":"2024-09-22T19:24:08.505902Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\nOriginal Dataset Summary:\nTotal number of samples: 400915\nNumber of unique patients: 1042\nNumber of positive cases: 393\nNumber of negative cases: 400522\nRatio of negative to positive cases: 1019.14:1\n","output_type":"stream"}]},{"cell_type":"code","source":"#keep all positives\ndf_target_1 = df_train[df_train['target'] == 1]\n\n#just use 1% of negatives\ndf_target_0 = df_train[df_train['target'] == 0].sample(frac=0.01, random_state=42)\n\ndf_train_balanced = pd.concat([df_target_1, df_target_0]).reset_index(drop=True)\n\n# Print balanced dataset summary\nprint(\"Balanced Dataset Summary:\")\nprint(f\"Total number of samples: {len(df_train)}\")\nprint(f\"Number of unique patients: {df_train['patient_id'].nunique()}\")\n\npositive_cases = df_train_balanced['target'].sum()\ntotal_cases = len(df_train_balanced)\npositive_ratio = positive_cases / total_cases\n\nprint(f\"Number of positive cases: {positive_cases}\")\nprint(f\"Number of negative cases: {total_cases - positive_cases}\")\nprint(f\"New ratio of negative to positive cases: {(total_cases - positive_cases) / positive_cases:.2f}:1\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:08.511507Z","iopub.execute_input":"2024-09-22T19:24:08.511841Z","iopub.status.idle":"2024-09-22T19:24:08.977257Z","shell.execute_reply.started":"2024-09-22T19:24:08.511805Z","shell.execute_reply":"2024-09-22T19:24:08.975990Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Balanced Dataset Summary:\nTotal number of samples: 400915\nNumber of unique patients: 1042\nNumber of positive cases: 393\nNumber of negative cases: 4005\nNew ratio of negative to positive cases: 10.19:1\n","output_type":"stream"}]},{"cell_type":"code","source":"# df_test[feature_cols] = df_test[feature_cols].astype(np.float32) # cast inputs to float32\n\n# df_test.clip(df_train[feature_cols].astype(np.float32).min(), df_train[feature_cols].astype(np.float32).max(), axis=1)\n\nnum_feature_cols = df_train[feature_cols].select_dtypes(exclude='category').columns\n\ndf_test[num_feature_cols] = df_test[num_feature_cols].astype(np.float64).clip(\n    lower=df_train_balanced[num_feature_cols].astype(np.float64).min(), \n    upper=df_train_balanced[num_feature_cols].astype(np.float64).max(), \n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:08.978826Z","iopub.execute_input":"2024-09-22T19:24:08.979357Z","iopub.status.idle":"2024-09-22T19:24:09.266694Z","shell.execute_reply.started":"2024-09-22T19:24:08.979301Z","shell.execute_reply":"2024-09-22T19:24:09.265560Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit the scaler on the training data and transform it\ndf_train_balanced[feature_cols] = scaler.fit_transform(df_train_balanced[feature_cols])\n\n# Transform the specified feature columns in the test data using the same scaler\ndf_test[feature_cols] = scaler.transform(df_test[feature_cols])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:09.268076Z","iopub.execute_input":"2024-09-22T19:24:09.269103Z","iopub.status.idle":"2024-09-22T19:24:09.361977Z","shell.execute_reply.started":"2024-09-22T19:24:09.269058Z","shell.execute_reply":"2024-09-22T19:24:09.360876Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.nn import Parameter\n\ndef gem(x, p=3, eps=1e-4):\n    return F.avg_pool2d(x.clamp(min=eps), (x.size(-2), x.size(-1))).pow(1.0 / p)\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6, p_trainable=False):\n        super(GeM, self).__init__()\n        if p_trainable:\n            self.p = Parameter(torch.ones(1) * p)\n        else:\n            self.p = p\n        self.eps = eps\n\n\n    def forward(self, x):\n        ret = gem(x, p=self.p, eps=self.eps)\n        return ret\n\n\ndef get_activation(activ_name: str=\"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(inplace=True),\n        \"leakyReLU\" : nn.LeakyReLU(negative_slope=0.01, inplace = True),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    else:\n        raise NotImplementedError\n\n\nclass Conv2dBNActiv(nn.Module):\n    \"\"\"Conv2d -> (BN ->) -> Activation\"\"\"\n\n    def __init__(\n        self, in_channels, out_channels,\n        kernel_size, stride, padding,\n        bias=False, use_bn=True, activ=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(Conv2dBNActiv, self).__init__()\n        layers = []\n        layers.append(nn.Conv2d(\n            in_channels, out_channels,\n            kernel_size, stride, padding, bias=bias))\n        if use_bn:\n            layers.append(nn.BatchNorm2d(out_channels))\n\n        layers.append(get_activation(activ))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        return self.layers(x)\n\n\nclass SpatialAttentionBlock(nn.Module):\n    \"\"\"Spatial Attention for (C, H, W) feature maps\"\"\"\n\n    def __init__(\n        self, in_channels,\n        out_channels_list,\n    ):\n        \"\"\"Initialize\"\"\"\n        super(SpatialAttentionBlock, self).__init__()\n        self.n_layers = len(out_channels_list)\n        channels_list = [in_channels] + out_channels_list\n        assert self.n_layers > 0\n        assert channels_list[-1] == 1\n\n        for i in range(self.n_layers - 1):\n            in_chs, out_chs = channels_list[i: i + 2]\n            layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"relu\")\n            setattr(self, f\"conv{i + 1}\", layer)\n\n        in_chs, out_chs = channels_list[-2:]\n        layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"sigmoid\")\n        setattr(self, f\"conv{self.n_layers}\", layer)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = x\n        for i in range(self.n_layers):\n            h = getattr(self, f\"conv{i + 1}\")(h)\n\n        h = h * x\n        return h\n\nclass MaskGuidedAttention(nn.Module):\n    def __init__(self, in_dim, dropout_rate=0.0, num_heads=8, mask_influence=1.0):\n        super(MaskGuidedAttention, self).__init__()\n        self.num_heads = num_heads\n        self.in_dim = in_dim\n        self.head_dim = in_dim // num_heads\n        assert self.head_dim * num_heads == in_dim, \"in_dim must be divisible by num_heads\"\n\n        self.W_Q = nn.Linear(in_dim, in_dim)\n        self.W_K = nn.Linear(in_dim, in_dim)\n        self.W_V = nn.Linear(in_dim, in_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.gamma = nn.Parameter(torch.tensor(0.25))\n        self.alpha = nn.Parameter(torch.tensor(mask_influence))\n\n    def forward(self, x, mask):\n        # x: [seq_len, batch_size, in_dim]\n        # mask: [batch_size, channels, height, width]\n        seq_length, batch_size, _ = x.size()\n\n        # Reshape x to [batch_size, seq_length, in_dim]\n        x = x.permute(1, 0, 2)  # [batch_size, seq_length, in_dim]\n\n        # Compute spatial dimensions from seq_length\n        x_height = x_width = int(math.sqrt(seq_length))\n        if x_height * x_width != seq_length:\n            raise ValueError(f\"Computed spatial dimensions ({x_height}x{x_width}) do not match seq_length ({seq_length}).\")\n\n        # Resize mask to match x's spatial dimensions\n        mask = F.interpolate(mask, size=(x_height, x_width), mode='bilinear', align_corners=False)  # [batch_size, channels, x_height, x_width]\n\n        # If mask has multiple channels, reduce to single channel\n        if mask.size(1) > 1:\n            mask = mask.mean(dim=1, keepdim=True)  # [batch_size, 1, x_height, x_width]\n\n        # Flatten mask to [batch_size, seq_length]\n        mask = mask.view(batch_size, -1)\n\n        # Normalize and prepare the mask\n        mask = (mask - mask.mean(dim=1, keepdim=True)) / (mask.std(dim=1, keepdim=True) + 1e-5)\n        mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_length]\n        mask = mask.expand(-1, self.num_heads, seq_length, -1)  # [batch_size, num_heads, seq_length, seq_length]\n\n        # Linear projections for Q, K, V\n        Q = self.W_Q(x)\n        K = self.W_K(x)\n        V = self.W_V(x)\n\n        # Reshape for multi-head attention\n        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Compute attention scores\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # Integrate mask into attention scores\n        attn_scores = attn_scores + (self.alpha * mask)\n\n        # Compute attention weights\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Compute attention output\n        attn_output = torch.matmul(attn_weights, V)\n\n        # Concatenate heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.in_dim)\n\n        # Residual connection\n        out = self.gamma * attn_output + x\n\n        # Permute back to original shape [seq_len, batch_size, in_dim]\n        out = out.permute(1, 0, 2)\n\n        return out\n\n\n\n\n\nclass SpatialAttentionBlock2(nn.Module):\n    def __init__(self, in_channels):\n        super(SpatialAttentionBlock2, self).__init__()\n        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1)  # Added to match original channels\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        attn = torch.cat([avg_out, max_out], dim=1)\n        attn = self.conv1(attn)\n        attn = self.sigmoid(attn)\n\n        # Apply the attention mask to the original input\n        return self.conv2(attn * x)\n\nclass EnhancedSegmentationHead(nn.Module):\n    def __init__(self, in_channels, mid_channels=288, out_channels=1, dropout_prob=0.1):\n        super(EnhancedSegmentationHead, self).__init__()\n\n        # Downsample block 1\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n        self.norm1 = nn.GroupNorm(32, mid_channels)  # Using GroupNorm instead of InstanceNorm\n        self.mish1 = nn.Mish(inplace=True)\n        self.dropout1 = nn.Dropout2d(0.15)\n\n        # Downsample block 2\n        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1)\n        self.norm2 = nn.GroupNorm(32, mid_channels)\n        self.mish2 = nn.Mish(inplace=True)\n        self.dropout2 = nn.Dropout2d(0.05)\n\n        # Upsample block 1\n        self.upsample1 = nn.ConvTranspose2d(mid_channels, mid_channels // 2, kernel_size=2, stride=2)\n        self.norm3 = nn.GroupNorm(16, mid_channels // 2)\n        self.mish3 = nn.Mish(inplace=True)\n        self.dropout3 = nn.Dropout2d(0.05)\n\n        # Upsample block 2\n        self.upsample2 = nn.ConvTranspose2d(mid_channels // 2, mid_channels // 4, kernel_size=2, stride=2)\n        self.norm4 = nn.GroupNorm(8, mid_channels // 4)\n        self.mish4 = nn.Mish(inplace=True)\n        self.dropout4 = nn.Dropout2d(0.1)\n\n        # Spatial Attention\n        self.attention = SpatialAttentionBlock2(in_channels // 8)\n\n        # Final convolution layer to reduce channels\n        self.conv_out = nn.Conv2d(in_channels // 8, out_channels, kernel_size=1) #72\n        self.dropout5 = nn.Dropout2d(0.5)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Downsampling path\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.mish1(x)\n        x = self.dropout1(x)\n\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.mish2(x)\n        x = self.dropout2(x)\n\n        # Upsampling path\n        x = self.upsample1(x)\n        x = self.norm3(x)\n        x = self.mish3(x)\n        x = self.dropout3(x)\n\n        x = self.upsample2(x)\n        x = self.norm4(x)\n        x = self.mish4(x)\n        x = self.attention(x)  # Apply attention before final conv layer\n\n        # Output\n        out = self.conv_out(x)\n        out = self.dropout5(out)\n        out = self.sigmoid(out)  # Final activation without dropout to ensure stability\n\n        return out\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:09.363706Z","iopub.execute_input":"2024-09-22T19:24:09.364372Z","iopub.status.idle":"2024-09-22T19:24:09.423279Z","shell.execute_reply.started":"2024-09-22T19:24:09.364326Z","shell.execute_reply":"2024-09-22T19:24:09.422154Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport timm\n\n\n# convnext_base.fb_in22k_ft_in1k\n# convnext_tiny.in12k_ft_in1k\n# tiny_vit_21m_224.dist_in22k_ft_in1k\n\nclass ISICModel(nn.Module):\n    def __init__(self,\n                 backbone='tiny_vit_21m_224.dist_in22k_ft_in1k',\n                 num_classes=2,\n                 pretrained=False,\n                 freeze_base_model=False):\n        super(ISICModel, self).__init__()\n\n        # Encoder setup with the specified backbone\n        self.encoder = timm.create_model(\n            backbone,\n            pretrained=pretrained,\n        )\n\n        # Freeze the encoder if required\n        if freeze_base_model:\n            self.freeze_encoder(True)\n\n        self.nb_fts = self.encoder.num_features\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.spatial_attention = SpatialAttentionBlock(in_channels=self.nb_fts, out_channels_list=[self.nb_fts // 2, 1])\n        self.dropout = nn.Dropout2d(0.0)\n\n        self.segmentation_head = EnhancedSegmentationHead(in_channels=self.nb_fts)\n        self.gem_pooling = GeM(p=3, p_trainable=True)\n        self.mask_guided_attention = MaskGuidedAttention(\n            in_dim=self.nb_fts,\n            dropout_rate=0.0,\n            num_heads=8,\n            mask_influence=1.00\n        )\n\n        # Embedding layers for categorical features\n#         self.embeddings = nn.ModuleList([\n#             nn.Embedding(num_categories, min(50, (num_categories + 1) // 2))\n#             for num_categories in categorical_cardinalities\n#         ])\n\n        # Tabular feature processing network\n        self.tabular_net = nn.Sequential(\n            nn.Linear(124, 512),  # Assuming 82 tabular features\n            nn.BatchNorm1d(512),\n            nn.SiLU(),  # Activation changed to Mish\n            nn.Dropout(0.3),  # Dropout increased to 0.5\n            nn.Linear(512, 128),\n            nn.BatchNorm1d(128),\n            nn.SiLU()  # Activation changed to Mish\n            # nn.Dropout(0.3),  # Dropout increased to 0.5\n        )\n\n        # Combined classification head\n        self.head = nn.Sequential(\n            nn.Linear(self.nb_fts + 128, 128),  # Concatenate image features and tabular features\n            # nn.BatchNorm1d(128),\n            # nn.Mish(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, image, tabular_data):\n            # Image Path\n            x = self.encoder.forward_features(image)\n            x = self.spatial_attention(x)\n            x = self.dropout(x)\n            seg_mask = self.segmentation_head(x)\n\n            # Classification Path\n            cls_x = x.flatten(2).permute(2, 0, 1)  # Reshape to [seq_len, batch, nb_fts] for attention\n            cls_x = self.mask_guided_attention(cls_x, seg_mask)\n            cls_x = self.gem_pooling(x)\n            cls_x = cls_x.flatten(1)\n\n            # Tabular Path\n            tabular_feat = self.tabular_net(tabular_data)\n\n            # Concatenate Image and Tabular Features\n            combined_features = torch.cat((cls_x, tabular_feat), dim=1)\n\n            # Final Classification\n            output = self.head(combined_features)\n\n            return output, seg_mask\n\n    def freeze_encoder(self, flag):\n        for param in self.encoder.parameters():\n            param.requires_grad = not flag\n\nmodel = ISICModel()\n\n# Separate the parameters into different groups\nsegmentation_params = list(model.segmentation_head.parameters())\nclassification_params = list(model.head.parameters())\nencoder_params = list(model.encoder.parameters())\nattention_params = list(model.spatial_attention.parameters()) + \\\n                   list(model.gem_pooling.parameters()) + \\\n                   list(model.mask_guided_attention.parameters())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:09.424881Z","iopub.execute_input":"2024-09-22T19:24:09.425501Z","iopub.status.idle":"2024-09-22T19:24:10.368748Z","shell.execute_reply.started":"2024-09-22T19:24:09.425448Z","shell.execute_reply":"2024-09-22T19:24:10.367724Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def setup_isic_model(backbone='tiny_vit_21m_224.dist_in22k_ft_in1k', num_classes=2, freeze_base_model=False, pretrained=True):\n    model = ISICModel(backbone=backbone, num_classes=num_classes, pretrained=pretrained, freeze_base_model=freeze_base_model)\n    return model.to(device)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for name, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.370217Z","iopub.execute_input":"2024-09-22T19:24:10.370795Z","iopub.status.idle":"2024-09-22T19:24:10.377779Z","shell.execute_reply.started":"2024-09-22T19:24:10.370751Z","shell.execute_reply":"2024-09-22T19:24:10.376955Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class ISICDataset(Dataset):\n    def __init__(self, hdf5_file, isic_ids, tabular_features=None, targets=None, transform=None):\n        self.hdf5_file = hdf5_file\n        self.isic_ids = isic_ids\n        self.tabular_features = tabular_features  # Add tabular features here\n        self.targets = targets\n        self.transform = transform\n\n    def get_labels(self):\n        return self.targets\n\n    def __len__(self):\n        return len(self.isic_ids)\n\n    def __getitem__(self, idx):\n        with h5py.File(self.hdf5_file, 'r') as f:\n            img_bytes = f[self.isic_ids[idx]][()]\n\n        img = Image.open(io.BytesIO(img_bytes))\n        img = np.array(img)  # Convert PIL Image to numpy array\n\n        if self.transform:\n            transformed = self.transform(image=img)\n            img = transformed['image']\n\n        tabular_feat = None\n        if self.tabular_features is not None:\n            tabular_feat = self.tabular_features[idx]  # Get the tabular features for this index\n\n        if self.targets is not None:\n            target = self.targets[idx]\n        else:\n            target = torch.tensor(-1)  # Dummy target for test set\n\n        return img, tabular_feat, target  # Return image, tabular features, and target\n\n# Prepare Augmentations\naug_transform = A.Compose([\n\n    A.RandomRotate90(),\n    A.Flip(),\n    A.ShiftScaleRotate(shift_limit=0.0, scale_limit=0.15, rotate_limit=90, p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.18, contrast_limit=0.12, p=0.5),\n    A.OpticalDistortion(distort_limit=0.5, shift_limit=0.0, p=0.7),\n\n    A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=5, val_shift_limit=1, p=0.5),\n\n    A.ElasticTransform(alpha=0.2, sigma=6.0, p=0.5),\n    A.Resize(224, 224, interpolation=cv2.INTER_CUBIC),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\nbase_transform = A.Compose([\n    A.Resize(224, 224,interpolation=cv2.INTER_CUBIC),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.379045Z","iopub.execute_input":"2024-09-22T19:24:10.379589Z","iopub.status.idle":"2024-09-22T19:24:10.400561Z","shell.execute_reply.started":"2024-09-22T19:24:10.379547Z","shell.execute_reply":"2024-09-22T19:24:10.399740Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\ndef visualize_augmentations_positive(dataset, num_samples=3, num_augmentations=5, figsize=(20, 10)):\n    # Find positive samples\n    positive_samples = []\n    for i in range(len(dataset)):\n        _, label = dataset[i]\n        if label == 1:  # Assuming 1 is the positive class\n            positive_samples.append(i)\n\n        if len(positive_samples) == num_samples:\n            break\n\n    if len(positive_samples) < num_samples:\n        print(f\"Warning: Only found {len(positive_samples)} positive samples.\")\n\n    fig, axes = plt.subplots(num_samples, num_augmentations + 1, figsize=figsize)\n    fig.suptitle(\"Original and Augmented Versions of Positive Samples\", fontsize=16)\n\n    for sample_num, sample_idx in enumerate(positive_samples):\n        # Get a single sample\n        original_image, label = dataset[sample_idx]\n\n        # If the image is already a tensor (due to ToTensorV2 in the transform), convert it back to numpy\n        if isinstance(original_image, torch.Tensor):\n            original_image = original_image.permute(1, 2, 0).numpy()\n\n        # Reverse the normalization\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        original_image = (original_image * std + mean) * 255\n        original_image = original_image.astype(np.uint8)\n\n        # Display original image\n        axes[sample_num, 0].imshow(original_image)\n        axes[sample_num, 0].axis('off')\n        axes[sample_num, 0].set_title(\"Original\", fontsize=10)\n\n        # Apply and display augmentations\n        for aug_num in range(num_augmentations):\n            augmented = dataset.transform(image=original_image)['image']\n            # If the result is a tensor, convert it back to numpy\n            if isinstance(augmented, torch.Tensor):\n                augmented = augmented.permute(1, 2, 0).numpy()\n            # Reverse the normalization\n            augmented = (augmented * std + mean) * 255\n            augmented = augmented.astype(np.uint8)\n\n            axes[sample_num, aug_num + 1].imshow(augmented)\n            axes[sample_num, aug_num + 1].axis('off')\n            axes[sample_num, aug_num + 1].set_title(f\"Augmented {aug_num + 1}\", fontsize=10)\n\n    plt.tight_layout()\n    plt.show()\n\naugtest_dataset = ISICDataset(\n    hdf5_file=TRAIN_HDF5_FILE_PATH,\n    isic_ids=df_train['isic_id'].values,\n    targets=df_train['target'].values,\n    transform=aug_transform,\n)\n\n# visualize_augmentations_positive(augtest_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.401854Z","iopub.execute_input":"2024-09-22T19:24:10.402449Z","iopub.status.idle":"2024-09-22T19:24:10.420580Z","shell.execute_reply.started":"2024-09-22T19:24:10.402407Z","shell.execute_reply":"2024-09-22T19:24:10.419584Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, roc_auc_score\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80) -> float:\n\n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n\n    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n    v_gt = abs(np.asarray(solution.values)-1)\n\n    # flip the submissions to their compliments\n    v_pred = -1.0*np.asarray(submission.values)\n\n    max_fpr = abs(1-min_tpr)\n\n    # using sklearn.metric functions: (1) roc_curve and (2) auc\n    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n    if max_fpr is None or max_fpr == 1:\n        return auc(fpr, tpr)\n    if max_fpr <= 0 or max_fpr > 1:\n        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n\n    # Add a single point at max_fpr by linear interpolation\n    stop = np.searchsorted(fpr, max_fpr, \"right\")\n    x_interp = [fpr[stop - 1], fpr[stop]]\n    y_interp = [tpr[stop - 1], tpr[stop]]\n    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n    fpr = np.append(fpr[:stop], max_fpr)\n    partial_auc = auc(fpr, tpr)\n\n    return(partial_auc)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.421983Z","iopub.execute_input":"2024-09-22T19:24:10.422353Z","iopub.status.idle":"2024-09-22T19:24:10.436823Z","shell.execute_reply.started":"2024-09-22T19:24:10.422315Z","shell.execute_reply":"2024-09-22T19:24:10.435845Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, gamma=0.25, smooth=1e-6):\n        super(CombinedLoss, self).__init__()\n        self.dice_loss = DiceLoss(smooth)\n        self.cross_entropy = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.25]),label_smoothing=0.1) # weight=torch.tensor([1.0, 1.25]\n        self.gamma = gamma\n\n    def forward(self, logits, seg_mask, class_targets):\n        # Self-supervised segmentation part\n        pseudo_targets = generate_pseudo_targets(seg_mask)\n        seg_loss = self.dice_loss(seg_mask, pseudo_targets)\n\n        # Classification part\n        class_loss = self.cross_entropy(logits, class_targets)\n\n        # Combine losses\n        loss = self.gamma * seg_loss + 0.75 * class_loss\n        return loss\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, logits, targets):\n        probs = torch.sigmoid(logits)\n        num = probs * targets  # Element-wise multiplication between the predicted and true masks\n        num = 2 * torch.sum(num, dim=(1, 2, 3))  # Sum over all dimensions except batch size\n\n        den = probs + targets  # Element-wise addition\n        den = torch.sum(den, dim=(1, 2, 3))  # Sum over all dimensions except batch size\n\n        dice = (num + self.smooth) / (den + self.smooth)\n        return 1 - dice.mean()  # Dice Loss is 1 - Dice Coefficient\n\ndef generate_pseudo_targets(seg_mask):\n    # Generate pseudo-targets from the segmentation mask (this can be the model's own predictions)\n    return torch.sigmoid(seg_mask)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.440336Z","iopub.execute_input":"2024-09-22T19:24:10.440693Z","iopub.status.idle":"2024-09-22T19:24:10.456782Z","shell.execute_reply.started":"2024-09-22T19:24:10.440653Z","shell.execute_reply":"2024-09-22T19:24:10.455982Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, SequentialSampler, RandomSampler\nfrom torch.autograd import Variable\n\n# def train_evaluate(model, train_loader, val_loader, criterion, optimizer, scheduler, device):\n#     scaler = GradScaler()\n\n#     # Training phase\n#     model.train()\n#     for inputs, targets in tqdm(train_loader, desc=\"Training\"):\n#         inputs, targets = inputs.to(device), targets.to(device)\n#         optimizer.zero_grad(set_to_none=True)\n\n#         with autocast():\n#             outputs = model(inputs)\n#             loss = criterion(outputs, targets)\n\n#         scaler.scale(loss).backward()\n#         scaler.step(optimizer)\n#         scaler.update()\n\n#     # Evaluation phase\n#     model.eval()\n#     val_targets, val_outputs = [], []\n#     with torch.no_grad(), autocast():\n#         for inputs, targets in tqdm(val_loader, desc=\"Evaluating\"):\n#             inputs, targets = inputs.to(device), targets.to(device)\n#             outputs = model(inputs)\n#             val_targets.append(targets.cpu())\n#             val_outputs.append(outputs.softmax(dim=1)[:, 1].cpu())\n\n#     scheduler.step()\n#     return torch.cat(val_targets).numpy(), torch.cat(val_outputs).numpy()\n\ndef train_evaluate(model, train_loader, val_loader, criterion, optimizer, scheduler, device):\n    scaler = GradScaler()  # Updated from the deprecated version\n\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    for images, tabular_data, class_targets in tqdm(train_loader, desc=\"Training\"):\n        images, tabular_data, class_targets = images.to(device), tabular_data.to(device).float(), class_targets.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast():  # Updated to the new autocast syntax\n            outputs, seg_mask = model(images, tabular_data)  # Pass both images and tabular data\n            loss = criterion(outputs, seg_mask, class_targets)  # Pass seg_mask to criterion\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_value_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)  # Calculate average loss for training\n\n    # Evaluation phase\n    model.eval()\n    val_loss = 0.0\n    val_targets, val_outputs = [], []\n    with torch.no_grad(), autocast():  # Updated to the new autocast syntax\n        for images, tabular_data, class_targets in tqdm(val_loader, desc=\"Evaluating\"):\n            images, tabular_data, class_targets = images.to(device), tabular_data.to(device).float(), class_targets.to(device)\n            outputs, seg_mask = model(images, tabular_data)  # Pass both images and tabular data\n            loss = criterion(outputs, seg_mask, class_targets)  # Pass seg_mask to criterion\n            val_loss += loss.item()\n            val_targets.append(class_targets.cpu())\n            val_outputs.append(outputs.softmax(dim=1)[:, 1].cpu())\n\n    avg_val_loss = val_loss / len(val_loader)  # Calculate average loss for validation\n\n    scheduler.step()\n\n    return torch.cat(val_targets).numpy(), torch.cat(val_outputs).numpy(), avg_train_loss, avg_val_loss\n\ndef cross_validation_train(df_train, num_folds, num_epochs, hdf5_file_path, aug_transform, base_transform, device):\n    # Define the combined loss function for Cross Entropy and Dice Loss\n    criterion = CombinedLoss(gamma=0.25).to(device)\n    best_overall_targets, best_overall_outputs = None, None  # Best results across all folds\n\n    for fold in range(num_folds):\n        print(f\"\\nFold {fold + 1}/{num_folds}\")\n\n        # Split data for the current fold\n        train_df = df_train[df_train['fold'] != fold]\n        val_df = df_train[df_train['fold'] == fold]\n\n        # Create datasets and data loaders\n        train_dataset = ISICDataset(hdf5_file_path, train_df['isic_id'].values, train_df[feature_cols].values, train_df['target'].values, aug_transform)\n        val_dataset = ISICDataset(hdf5_file_path, val_df['isic_id'].values, val_df[feature_cols].values, val_df['target'].values, base_transform)\n\n        labels = train_dataset.get_labels()\n        class_weights = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels))\n\n        samples_weights = class_weights[labels]\n        print(class_weights)\n        sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n\n        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=sampler, num_workers=8, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=8, pin_memory=True)\n\n        # Initialize model, optimizer, and scheduler once per fold\n        model = setup_isic_model().to(device)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-5)\n        # optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.9, weight_decay=1e-5)\n        # Define the optimizer with different learning rates for each parameter group\n        # optimizer = torch.optim.AdamW([\n        #     {'params': segmentation_params, 'lr': 0.003},  # Learning rate for segmentation task\n        #     {'params': classification_params, 'lr': 0.0009}  # Learning rate for classification task\n        # ], weight_decay=1e-5)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n        best_score = float('-inf')  # Initialize the best score to negative infinity\n        best_model_path = f'model_fold_{fold + 1}_best.pth'  # Path to save the best model for this fold\n\n        best_val_targets, best_val_outputs = None, None  # Best targets and outputs for this fold\n\n        for epoch in range(num_epochs):\n            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n\n            print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, \"\n                  f\"Train Pos Ratio: {train_df['target'].mean():.2%}, Val Pos Ratio: {val_df['target'].mean():.2%}\")\n\n            # Train and evaluate\n            val_targets, val_outputs, avg_train_loss, avg_val_loss = train_evaluate(\n                model, train_loader, val_loader, criterion, optimizer, scheduler, device)\n\n            print(f'Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n\n            # Create DataFrames with row_id for scoring\n            solution_df = pd.DataFrame({'target': val_targets, 'row_id': range(len(val_targets))})\n            submission_df = pd.DataFrame({'prediction': val_outputs, 'row_id': range(len(val_outputs))})\n            fold_score = score(solution_df, submission_df, 'row_id')\n            print(f'Fold {fold + 1}, Epoch {epoch + 1} pAUC Score: {fold_score:.4f}')\n\n            # Save the model and best results if this is the best score for this fold\n            if fold_score > best_score:\n                best_score = fold_score\n                best_val_targets = val_targets  # Save the best targets\n                best_val_outputs = val_outputs  # Save the best outputs\n                torch.save(model.state_dict(), best_model_path)\n                print(f\"New best model saved for fold {fold + 1} with pAUC Score: {best_score:.4f}\")\n\n        # After all epochs for the current fold, store the best results\n        if best_val_targets is not None and best_val_outputs is not None:\n            if best_overall_targets is None and best_overall_outputs is None:\n                best_overall_targets = best_val_targets\n                best_overall_outputs = best_val_outputs\n            else:\n                best_overall_targets = np.concatenate([best_overall_targets, best_val_targets])\n                best_overall_outputs = np.concatenate([best_overall_outputs, best_val_outputs])\n\n    print(f'\\nBest models saved for each fold.')\n\n    # Return the best accumulated targets and outputs for final evaluation\n    return np.array(best_overall_targets), np.array(best_overall_outputs)\n\n\n# # Set up CUDA if available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # Perform cross-validation training and get the accumulated results\n# all_val_targets, all_val_outputs = cross_validation_train(df_train_balanced, num_folds, num_epochs, TRAIN_HDF5_FILE_PATH, aug_transform, base_transform, device)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.459326Z","iopub.execute_input":"2024-09-22T19:24:10.459697Z","iopub.status.idle":"2024-09-22T19:24:10.494168Z","shell.execute_reply.started":"2024-09-22T19:24:10.459661Z","shell.execute_reply":"2024-09-22T19:24:10.493234Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# # Final overall evaluation\n# print(\"\\nFinal Overall Evaluation:\")\n\n# # Calculate the official pAUC score\n# solution_df = pd.DataFrame({'target': all_val_targets, 'row_id': range(len(all_val_targets))})\n# submission_df = pd.DataFrame({'prediction': all_val_outputs, 'row_id': range(len(all_val_outputs))})\n# official_score = score(solution_df, submission_df, 'row_id')\n# print(f'Overall pAUC Score: {official_score:.4f}')\n\n# # Generate and print classification report\n# binary_predictions = binarize(np.array(all_val_outputs).reshape(-1, 1), threshold=0.5).reshape(-1)\n# report = classification_report(all_val_targets, binary_predictions, target_names=['Class 0', 'Class 1'])\n# print(\"\\nOverall Classification Report:\")\n# print(report)\n\n# # Print specific metrics for Class 1\n# report_dict = classification_report(all_val_targets, binary_predictions, target_names=['Class 0', 'Class 1'], output_dict=True)\n# print(f\"\\nClass 1 Metrics:\")\n# print(f\"Precision: {report_dict['Class 1']['precision']:.4f}\")\n# print(f\"Recall: {report_dict['Class 1']['recall']:.4f}\")\n# print(f\"F1-score: {report_dict['Class 1']['f1-score']:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.495438Z","iopub.execute_input":"2024-09-22T19:24:10.495807Z","iopub.status.idle":"2024-09-22T19:24:10.506781Z","shell.execute_reply.started":"2024-09-22T19:24:10.495775Z","shell.execute_reply":"2024-09-22T19:24:10.505968Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# df_train['age_approx'] = df_train['age_approx'].astype('float64')\n\n# # Calculate the mean of 'age_approx' from the training dataset, ignoring NaN values\n# mean_age = df_train['age_approx'].mean()\n\n# # Fill NaN values in 'age_approx' with the mean\n# df_test['age_approx'].fillna(mean_age, inplace=True)\n# df_test['age_approx'] = df_test['age_approx'].astype('float64')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.508197Z","iopub.execute_input":"2024-09-22T19:24:10.508517Z","iopub.status.idle":"2024-09-22T19:24:10.517903Z","shell.execute_reply.started":"2024-09-22T19:24:10.508486Z","shell.execute_reply":"2024-09-22T19:24:10.516963Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport h5py\nimport timm\nfrom torchvision import transforms\nfrom PIL import Image\nimport io\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport gc\n\nepoch_for_preds = num_epochs\nmodel_path = \"/kaggle/input/newest-isic-with-maskguide-corrected/\"\n\nclass ISICDataset(Dataset):\n    def __init__(self, hdf5_file, isic_ids, tabular_features=None, targets=None, transform=None):\n        self.hdf5_file = h5py.File(hdf5_file, 'r')  # Keep file open\n        self.isic_ids = isic_ids\n        self.tabular_features = tabular_features\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.isic_ids)\n\n    def __getitem__(self, idx):\n    # Use self.hdf5_file directly since it's opened in __init__\n        img_bytes = self.hdf5_file[self.isic_ids[idx]][()]\n        img = Image.open(io.BytesIO(img_bytes))\n        img = np.array(img)\n\n        if self.transform:\n            transformed = self.transform(image=img)\n            img = transformed['image']\n\n        tabular_feat = None\n        if self.tabular_features is not None:\n            tabular_feat = self.tabular_features[idx]\n\n        target = self.targets[idx] if self.targets is not None else torch.tensor(-1)\n        return img, tabular_feat, target, self.isic_ids[idx]\n\n\n    def __del__(self):\n        self.hdf5_file.close()  # Ensure file is closed when object is destroyed\n\n# Define the albumentations transformation\nbase_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\ndef setup_isic_model(backbone='tiny_vit_21m_224.dist_in22k_ft_in1k', num_classes=2, freeze_base_model=False, pretrained=False):\n    model = ISICModel(backbone=backbone, num_classes=num_classes, pretrained=pretrained, freeze_base_model=freeze_base_model)\n    return model.to(device)\n\n\ndef load_model(fold, device):\n    model = setup_isic_model().to(device)\n    model.load_state_dict(torch.load(f'{model_path}model_fold_{fold}_best (1).pth', map_location=device))\n    model.eval()\n    return model\n\n@torch.no_grad()  # Apply no_grad to the entire function\ndef predict_for_fold_in_batches(model, test_loader, fold, device, batch_size=256):\n    with open(f'predictions_fold_{fold}.csv', 'w') as f_out:\n        f_out.write(f'isic_id,fold_{fold}_pred\\n')\n\n        for batch_idx, (images, tabular_data, targets, isic_ids) in enumerate(tqdm(test_loader, desc=f\"Predicting Fold {fold}\")):\n            images = images.to(device)\n            if tabular_data is not None:\n                tabular_data = tabular_data.to(device).float()\n\n            # Forward pass\n            output, _ = model(images, tabular_data)\n            predictions = output.softmax(dim=1)[:, 1].cpu().numpy()\n\n            for isic_id, prediction in zip(isic_ids, predictions):\n                f_out.write(f\"{isic_id},{prediction:.4f}\\n\")\n\n            # Free up memory for this batch\n            del images, tabular_data, output, predictions\n            torch.cuda.empty_cache()\n            gc.collect()\n\n            # Add torch synchronization to ensure GPU processes finish before moving on\n            torch.cuda.synchronize()\n            \ndef ensemble_predict_in_batches(folds, test_loader, device, batch_size=256):  # Adjust batch size\n    for fold in folds:\n        print(f\"Processing fold {fold}...\")\n\n        # Load model for the current fold\n        model = load_model(fold, device)\n\n        # Run predictions for the current fold in batches\n        predict_for_fold_in_batches(model, test_loader, fold, device, batch_size)\n\n        # Free up memory by deleting the model after each fold\n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        \ndef average_fold_predictions_in_chunks(folds, df_test, chunk_size=2000):\n    with open('submission.csv', 'w') as f_out:\n        # Write header for the final submission file\n        f_out.write('isic_id,target\\n')\n\n        # Create readers for each fold's CSV in chunks\n        readers = [pd.read_csv(f'predictions_fold_{fold}.csv', chunksize=chunk_size) for fold in folds]\n\n        for chunk_idx, chunk_sets in enumerate(zip(*readers)):\n            df_chunk_avg = pd.DataFrame()\n\n            for fold_idx, df_chunk in enumerate(chunk_sets):\n                if df_chunk_avg.empty:\n                    df_chunk_avg = df_chunk[['isic_id']].copy()  # Initialize with isic_id\n                    df_chunk_avg['target'] = df_chunk[f'fold_{folds[fold_idx]}_pred']  # Start with first fold's prediction\n                else:\n                    df_chunk_avg['target'] += df_chunk[f'fold_{folds[fold_idx]}_pred']  # Sum the predictions\n\n            # Average the predictions by dividing by the number of folds\n            df_chunk_avg['target'] /= len(folds)\n\n            # Round the averaged predictions to 4 decimal places\n            df_chunk_avg['target'] = df_chunk_avg['target'].round(8)\n\n            # Write the averaged and rounded chunk to the final CSV\n            df_chunk_avg.to_csv(f_out,header=False, index=False)\n\n            # Clean up memory after each chunk\n            del df_chunk_avg\n            gc.collect()\n\n    print(\"Final submission file created successfully.\")\n\n# def ensemble_predict(models, test_loader, device):\n#     all_predictions = []\n#     all_seg_masks = []  # To store segmentation masks, if needed\n\n#     for images, tabular_data, _ in tqdm(test_loader, desc=\"Predicting\"):\n#         # Move data to the appropriate device\n#         images, tabular_data = images.to(device), tabular_data.to(device).float()\n        \n#         # Store predictions from all models (for ensemble predictions)\n#         fold_predictions = []\n#         fold_seg_masks = []\n\n#         for model in models:\n#             model.eval()\n#             # Get both class outputs and segmentation masks\n#             output, seg_mask = model(images, tabular_data)\n#             # Apply softmax to the outputs and get predictions for class 1\n#             fold_predictions.append(output.softmax(dim=1)[:, 1].cpu())\n#             fold_seg_masks.append(seg_mask.cpu())  # Assuming you want to store segmentation masks\n\n#         # Stack predictions from all folds and average them\n#         avg_predictions = torch.stack(fold_predictions).mean(dim=0)\n#         avg_seg_masks = torch.stack(fold_seg_masks).mean(dim=0)  # Averaging segmentation masks\n\n#         # Append to all_predictions\n#         all_predictions.extend(avg_predictions.numpy())\n#         all_seg_masks.extend(avg_seg_masks.numpy())\n\n#     return all_predictions, all_seg_masks\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.519397Z","iopub.execute_input":"2024-09-22T19:24:10.519690Z","iopub.status.idle":"2024-09-22T19:24:10.546832Z","shell.execute_reply.started":"2024-09-22T19:24:10.519659Z","shell.execute_reply":"2024-09-22T19:24:10.545914Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# def generate_oof_predictions(df_train, folds, hdf5_file_path, transform):\n#     oof_predictions = np.zeros(len(df_train))\n#     model_filenames = [''] * len(df_train)\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#     models = load_models(folds, device)\n\n#     for fold in folds:\n#         print(f\"Generating predictions for fold {fold}/{num_folds}\")\n#         val_df = df_train[df_train['fold'] == fold].copy()\n#         val_dataset = ISICDataset(hdf5_file_path, val_df['isic_id'].values, val_df[feature_cols].values, val_df['target'].values, transform)\n#         val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n\n#         fold_predictions = ensemble_predict([models[fold]], val_loader, device)\n\n#         oof_predictions[val_df.index] = fold_predictions\n#         model_filename = f'model_fold_{fold}_best_tinyvit.pth'\n#         for idx in val_df.index:\n#             model_filenames[idx] = model_filename\n\n#     return oof_predictions, model_filenames\n\n\n# # if not scoring:\n# #     # Set up CUDA if available\n# #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# #     print(f\"Using device: {device}\")\n\n# #     # Define the number of folds\n# #     folds = [1, 2, 3, 4, 5]\n\n# #     # Generate out-of-fold predictions\n# #     oof_predictions, model_filenames = generate_oof_predictions(df_train, folds, TRAIN_HDF5_FILE_PATH, base_transform)\n\n# #     # Create DataFrame for OOF predictions\n# #     oof_df = pd.DataFrame({\n# #         'isic_id': df_train['isic_id'],\n# #         'target': df_train['target'],\n# #         'fold': df_train['fold'],\n# #         'oof_prediction': oof_predictions,\n# #         'model_filename': model_filenames\n# #     })\n\n# #     # Save OOF predictions to CSV\n# #     oof_df.to_csv('oof_predictions.csv', index=False)\n# #     print(\"Out-of-fold predictions saved to oof_predictions.csv\")\n# #     print(oof_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.548232Z","iopub.execute_input":"2024-09-22T19:24:10.548586Z","iopub.status.idle":"2024-09-22T19:24:10.559680Z","shell.execute_reply.started":"2024-09-22T19:24:10.548553Z","shell.execute_reply":"2024-09-22T19:24:10.558803Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv(\"/kaggle/input/isic-2024-challenge/test-metadata.csv\")\nTEST_HDF5_FILE_PATH = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n\n# Set up CUDA if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Folds to use for prediction\nfolds = [1, 2, 3, 4, 5]\n\n# Prepare your test dataset\ntest_dataset = ISICDataset(\n    hdf5_file=TEST_HDF5_FILE_PATH,\n    isic_ids=df_test['isic_id'].values,\n    tabular_features=df_test[feature_cols].values,\n    transform=base_transform\n)\n\n# Create test data loader\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n\n# Perform predictions for each fold in batches\nensemble_predict_in_batches(folds, test_loader, device, batch_size=256)\n\n# After all fold predictions are made, average them in batches\nsubmission_df = average_fold_predictions_in_chunks(folds, df_test, chunk_size=2000)\n\n\n# Save the final averaged predictions to a CSV file\nsubmission_new_df=pd.read_csv('/kaggle/working/submission.csv')\nprint(\"Predictions saved to submission.csv\")\nprint(submission_new_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-22T19:24:10.560808Z","iopub.execute_input":"2024-09-22T19:24:10.561143Z","iopub.status.idle":"2024-09-22T19:24:19.236197Z","shell.execute_reply.started":"2024-09-22T19:24:10.561079Z","shell.execute_reply":"2024-09-22T19:24:19.235120Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Using device: cuda\nProcessing fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_218/1099898851.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f'{model_path}model_fold_{fold}_best (1).pth', map_location=device))\nPredicting Fold 1: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Processing fold 2...\n","output_type":"stream"},{"name":"stderr","text":"Predicting Fold 2: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing fold 3...\n","output_type":"stream"},{"name":"stderr","text":"Predicting Fold 3: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing fold 4...\n","output_type":"stream"},{"name":"stderr","text":"Predicting Fold 4: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing fold 5...\n","output_type":"stream"},{"name":"stderr","text":"Predicting Fold 5: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Final submission file created successfully.\nPredictions saved to submission.csv\n        isic_id   target\n0  ISIC_0015657  0.07590\n1  ISIC_0015729  0.04316\n2  ISIC_0015740  0.05394\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}